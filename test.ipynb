{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## <center>Practice Project Question I\n",
    "**1.** Download the data and read the descriptions in the file adult.names. Remove records with unknown (?) values from both train and test data sets and remove all continuous attributes. For each multi-domain categorical attribute, you can use one-hot encoding to transform data (this step is needed if you choose scikit-learn to build decision tree and naïve classifier; it is optional if you choose Weka). In your report, describe briefly how you develop your algorithm or apply software on the following two tasks and include 2-4 screenshots about your algorithm settings and output\n",
    "- Build a decision tree classifier (single tree) and report accuracy by class including (TP rate, FP rate, precision, recall, F1) on the test data.\n",
    "- Build a naïve Bayesian classifier and report accuracy by class including (TP rate, FP rate, precision, recall, F1) on the test data.\n",
    "## Training Data Cleaning\n",
    "# import pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# read in csv dataset\n",
    "train_df = pd.read_csv(\"dataset/adult_training.csv\")\n",
    "train_df\n",
    "# added column keys since there is none provided from original dataset\n",
    "train_df.columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \">50k\"]\n",
    "train_df.keys()\n",
    "### Dropping continuous attributes\n",
    "\n",
    "# dropping continuous attributes\n",
    "train_df.drop([\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"], axis = 1, inplace=True)\n",
    "train_df.head(20)\n",
    "### Removing all rows that contains \" ?\" in data\n",
    "# removing all rows that contains \" ?\" in data\n",
    "for col in train_df.columns:\n",
    "    train_df.drop(train_df.index[train_df[col] == \" ?\"], inplace=True)\n",
    "\n",
    "train_df.head(20)\n",
    "### One-hot encoding on training dataset\n",
    "# one-hot encoding on training dataset\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "train_df = train_df.apply(le.fit_transform)\n",
    "\n",
    "train_df\n",
    "## Testing Data Cleaning\n",
    "# loading in testing data\n",
    "testing_df = pd.read_csv(\"dataset/adult_test.csv\")\n",
    "# added column keys since there is none provided from original dataset\n",
    "testing_df.columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \">50k\"]\n",
    "### Dropping continuous attributes\n",
    "# dropping continuous attributes\n",
    "testing_df.drop([\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"], axis = 1, inplace=True)\n",
    "# removing all rows that contains \" ?\" in data\n",
    "for col in testing_df.columns:\n",
    "    testing_df.drop(testing_df.index[testing_df[col] == \" ?\"], inplace=True)\n",
    "### One-hot encoding\n",
    "# one-hot encoding on testing dataset\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "testing_df = testing_df.apply(le.fit_transform)\n",
    "\n",
    "testing_df\n",
    "### Loading Data into CSV\n",
    "#test data clean\n",
    "testing_df.to_csv('dataset/part1_data_clean/adult_test_clean.csv',index = False)\n",
    "\n",
    "train_df.to_csv('dataset/part1_data_clean/adult_train_clean.csv',index = False)\n",
    "## Build Decision Classifier(Single Tree)\n",
    "## code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# defining FEATURES | TARGET\n",
    "FEATURES = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "TARGET = \">50k\"\n",
    "\n",
    "# training set 90% from adult_training.csv\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df[FEATURES], train_df[TARGET], test_size=0.1, random_state=1)\n",
    "# testing set 90% from adult_test.csv\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(testing_df[FEATURES], testing_df[TARGET], test_size=0.9, random_state=1)\n",
    "\n",
    "# create and fit training data into decision tree classifier\n",
    "model = DecisionTreeClassifier()\n",
    "test = model.fit(X_train, y_train)\n",
    "\n",
    "# get prediction from model based on testing set\n",
    "y_pred = test.predict(X_test2)\n",
    "\n",
    "# output accuracy of model\n",
    "print(f'Accuracy: {metrics.accuracy_score(y_test2, y_pred)*100:.4f}%')\n",
    "## Decision Tree Accuracy\n",
    "- TP Rate\n",
    "## code for TP Rate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\"\"\" 2X2 Confusion Matrix\n",
    "TP | FN\n",
    "--- ---\n",
    "FP | TN\n",
    "\"\"\"\n",
    "\n",
    "CM = confusion_matrix(y_test2, y_pred)\n",
    "\n",
    "TP = CM[0][0]\n",
    "FN = CM[1][0]\n",
    "TN = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "\n",
    "\n",
    "print(f'True Positive: {TP}')\n",
    "- FP Rate\n",
    "## code fpr FP Rate\n",
    "CM = confusion_matrix(y_test2, y_pred)\n",
    "\n",
    "\"\"\" 2X2 Confusion Matrix\n",
    "TP | FN\n",
    "--- ---\n",
    "FP | TN\n",
    "\"\"\"\n",
    "\n",
    "TP = CM[0][0]\n",
    "FN = CM[1][0]\n",
    "TN = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "\n",
    "print(f'False Positive: {FP}')\n",
    "- Precision\n",
    "## code for Precision\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "#finding precision score with \"weighted\"\n",
    "precision = precision_score(y_test2, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "print(f'Precison: {precision*100:.4f}%')\n",
    "- Recall\n",
    "## code Recall\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# finding recall score with \"weighted\"\n",
    "recall = recall_score(y_test2, y_pred, average='weighted') \n",
    "\n",
    "\n",
    "print(f'Recall: {recall*100:.4f}%')\n",
    "- F1\n",
    "## code for F1\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# finding \"F1_Score\" with \"weighted\"\n",
    "scores = cross_val_score(\n",
    "    model,\n",
    "    X_train[FEATURES],\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring = make_scorer(f1_score, average ='weighted')\n",
    ")\n",
    "\n",
    "print(f'F1 Score: {scores.mean()*100:.4f}%')\n",
    "## Build Naïve Bayesian Classifier\n",
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FEATURES = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "TARGET = \">50k\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df[FEATURES], train_df[TARGET], test_size=0.1, random_state=1)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(testing_df[FEATURES], testing_df[TARGET], test_size=0.9, random_state=1)\n",
    "\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "bayModel = GaussianNB()\n",
    "bayModel.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "bay_pred = test.predict(X_test2)\n",
    "\n",
    "\n",
    "print(f\"Naïve Bayesian Accuracy: {metrics.accuracy_score(y_test2, bay_pred)*100:.4f}%\")\n",
    "## Naïve Bayesian Accuracy\n",
    "- TP Rate\n",
    "## code TP Rate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "CM = confusion_matrix(y_test2, bay_pred)\n",
    "\n",
    "TP = CM[0][0]\n",
    "FN = CM[1][0]\n",
    "TN = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "\n",
    "\n",
    "print(f'True Positive: {TP}')\n",
    "- FP Rate\n",
    "## code for FP Rate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "CM = confusion_matrix(y_test2, bay_pred)\n",
    "\n",
    "TP = CM[0][0]\n",
    "FN = CM[1][0]\n",
    "TN = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "\n",
    "\n",
    "print(f'False Positive: {FP}')\n",
    "- Precision\n",
    "## code for Precision\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "#finding precision score with \"weighted\"\n",
    "precision = precision_score(y_test2, bay_pred, average='weighted')\n",
    "\n",
    "print(f'Precison: {precision*100:.4f}%')\n",
    "- Recall\n",
    "## code for Recall\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# finding recall score with \"weighted\"\n",
    "recall = recall_score(y_test2, bay_pred, average='weighted') \n",
    "\n",
    "\n",
    "print(f'Recall: {recall*100:.4f}%')\n",
    "- F1\n",
    "## code for F1\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# finding \"F1_Score\" with \"weighted\"\n",
    "scores = cross_val_score(\n",
    "    bayModel,\n",
    "    X_train[FEATURES],\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring = make_scorer(f1_score, average ='weighted')\n",
    ")\n",
    "\n",
    "print(f'F1 Score: {scores.mean()*100:.4f}%')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
